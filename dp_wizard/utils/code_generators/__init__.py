import re
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from platform import python_version
from typing import Optional

import polars as pl
import yaml
from dp_wizard_templates.code_template import Template

from dp_wizard import __version__, opendp_version, registry_url
from dp_wizard.types import ColumnName, Product, StatisticName


class DefaultsTemplate(Template):
    def finish(self, reformat=False):
        self.fill_expressions(
            OPENDP_V_VERSION=f"v{opendp_version}",
            optional=True,
        )
        return super().finish(reformat)


@dataclass
class AnalysisPlanColumn:
    statistic_name: StatisticName
    lower_bound: float
    upper_bound: float
    bin_count: int
    weight: int

    def __str__(self) -> str:
        # For now, does not include weight or bin count.
        # If we add bin count, make sure to do it in a way
        # that is in sync with analyses.has_bins().
        return f"{self.statistic_name} clipped to {self.lower_bound}-{self.upper_bound}"


def _readable_type(t: pl.DataType):
    """
    Polars supports a range of numeric datatypes,
    but scan_csv() only returns a subset:
    >>> from io import BytesIO
    >>> import polars as pl
    >>> for k, v in (
    ...     pl.read_csv(BytesIO(b"b,i,f,s\\nTrue,1,1.0,abc"))
    ... ).schema.items():
    ...     print(k, v, _readable_type(v))
    b Boolean Boolean
    i Int64 Integer
    f Float64 Float
    s String String

    """
    if t.is_integer():
        return "Integer"
    elif t.is_numeric():
        return "Float"
    else:
        return str(t)


@dataclass
class AnalysisPlan:
    """
    >>> plan = AnalysisPlan(
    ...     product=Product.STATISTICS,
    ...     csv_path='optional.csv',
    ...     contributions=10,
    ...     contributions_entity='Family',
    ...     epsilon=2.0,
    ...     max_rows=1000,
    ...     groups={'grouping_col': ['expected', 'values']},
    ...     analysis_columns={
    ...         'data_col': [AnalysisPlanColumn('Histogram', 0, 100, 10, 1)]
    ...     },
    ...     schema_columns={
    ...         'data_col': pl.Int32(),
    ...         'grouping_col': pl.String(),
    ...         'unused_col': pl.Boolean(),
    ...     },
    ... )
    >>> print(plan)
    DP Statistics for `data_col` grouped by `grouping_col`
    >>> print(plan.get_stem())
    dp_statistics_for_data_col_grouped_by_grouping_col
    >>> print(plan.get_note())
    This demonstrates how to calculate ...
    Generated by DP Wizard ...
    <BLANKLINE>
    - Privacy Budget (Epsilon) = 2.0
    - Maximum of 10 rows per Family
    - Columns:
        - data_col (Integer): Histogram clipped to 0-100
        - grouping_col (String): "expected", "values"
        - unused_col (Boolean): Not used
    """

    product: Product
    csv_path: Optional[str]
    contributions: int
    contributions_entity: str
    epsilon: float
    max_rows: int
    groups: dict[ColumnName, list[str | float]]
    analysis_columns: dict[ColumnName, list[AnalysisPlanColumn]]
    schema_columns: dict[ColumnName, pl.DataType]

    def __str__(self) -> str:
        def md_list(names) -> str:
            return ", ".join(f"`{name}`" for name in names)

        columns_md = md_list(self.analysis_columns.keys()) or "TBD"
        groups_md = md_list(self.groups)
        grouped_by = f" grouped by {groups_md}" if groups_md else ""
        return f"{self.product} for {columns_md}{grouped_by}"

    def get_absolute_csv_path(self) -> str:
        if self.csv_path is None:
            return ""  # pragma: no cover
        return str(Path(self.csv_path).absolute())

    def get_stem(self) -> str:
        return re.sub(r"\W+", " ", str(self)).strip().replace(" ", "_").lower()

    def get_note(self) -> str:
        now = datetime.now().strftime("%b %d, %Y at %I:%M%p")
        columns = "\n".join(
            f"    - {k} ({_readable_type(v)}): "
            + (
                "; ".join(
                    # Analysis column info:
                    (
                        [str(a) for a in c]
                        if (c := self.analysis_columns.get(k)) is not None
                        else []
                    )
                    # Grouping column info:
                    + (
                        [", ".join(f'"{v}"' for v in c) or "Used for grouping"]
                        if (c := self.groups.get(k)) is not None
                        else []
                    )
                )
                or "Not used"
            )
            for k, v in self.schema_columns.items()
        )
        return f"""
This demonstrates how to calculate {self} using OpenDP (https://docs.opendp.org).
Generated by DP Wizard v{__version__} (https://github.com/opendp/dp-wizard) on {now}.

- Privacy Budget (Epsilon) = {self.epsilon}
- Maximum of {self.contributions} rows per {self.contributions_entity}
- Columns:
{columns}
        """.strip()

    def get_groups_with_keys(self) -> dict[ColumnName, list[str | float]]:
        # When AnalysisPlan is created, we confirm that groups are selected.
        # (We preserve the list of values, even if not currently selected.)
        # This selects the subset with public keys.
        return {k: v for k, v in self.groups.items() if v}

    def serialize(self):
        # NOTE: safe_dump does not work here: I think we do eventually
        # want to support round trips, and if we trust the CLI user that's ok.

        yaml.add_representer(
            Product,
            lambda dumper, data: dumper.represent_scalar("!Product", data.name),
        )
        yaml.add_representer(
            ColumnName,
            lambda dumper, data: dumper.represent_scalar("!ColumnName", data),
        )
        yaml.add_representer(
            StatisticName,
            lambda dumper, data: dumper.represent_scalar("!StatisticName", data),
        )
        # TODO: Add representers for objects.

        return yaml.dump(
            {
                "environment": {
                    "dp_wizard": __version__,
                    "python": python_version(),
                },
                "analysis_plan": self,
            }
        )


# Public functions used to generate code snippets in the UI;
# These do not require an entire analysis plan, so they stand on their own.


def make_privacy_unit_block(
    contributions: int,
    contributions_entity: str,
):
    import opendp.prelude as dp

    def template(CONTRIBUTIONS, CONTRIBUTIONS_ENTITY):
        # Each CONTRIBUTIONS_ENTITY can contribute this many rows.
        contributions = CONTRIBUTIONS
        privacy_unit = dp.unit_of(contributions=contributions)  # noqa: F841

    return (
        DefaultsTemplate(template)
        .fill_values(CONTRIBUTIONS=contributions)
        .fill_expressions(CONTRIBUTIONS_ENTITY=contributions_entity)
        .finish()
    )


def make_privacy_loss_block(pure: bool, epsilon: float, max_rows: int):
    """
    Comments in the *pure* privacy loss block reference synthetic data generation
    ("cuts dict"), so don't use "pure=True" for stats code!

    >>> print(
    ...     'pure DP: ',
    ...     make_privacy_loss_block(pure=True, epsilon=1, max_rows=1000)
    ... )
    pure DP: ...delta=0...
    >>> print(
    ...     'approx DP: ',
    ...     make_privacy_loss_block(pure=False, epsilon=1, max_rows=1000)
    ... )
    approx DP: ...delta=1 / max...
    """

    import opendp.prelude as dp

    if pure:

        def template(EPSILON, MAX_ROWS):
            privacy_loss = dp.loss_of(  # noqa: F841
                # EPSILON_COMMENT_BLOCK
                epsilon=EPSILON,
                # If your columns don't match your cuts dict,
                # you will also need to provide a very small "delta" value.
                # https://docs.opendp.org/en/OPENDP_V_VERSION/getting-started/tabular-data/grouping.html#Stable-Keys
                delta=0,  # or 1 / max(1e7, MAX_ROWS),
            )

    else:

        def template(EPSILON, MAX_ROWS):
            privacy_loss = dp.loss_of(  # noqa: F841
                # EPSILON_COMMENT_BLOCK
                epsilon=EPSILON,
                # There are many models of differential privacy. For flexibility,
                # we are using a model which tolerates a small probability (delta)
                # that data may be released in the clear. Delta should always be small,
                # but if the dataset is particularly large,
                # delta should be at least as small as 1/(row count).
                # https://docs.opendp.org/en/OPENDP_V_VERSION/getting-started/tabular-data/grouping.html#Stable-Keys
                delta=1 / max(1e7, MAX_ROWS),
            )

    return (
        DefaultsTemplate(template)
        .fill_values(
            EPSILON=epsilon,
            MAX_ROWS=max_rows,
        )
        .fill_blocks(
            EPSILON_COMMENT_BLOCK=f"""
Your privacy budget is captured in the "epsilon" parameter.
Larger values increase the risk that personal data could be
reconstructed, so choose the smallest value that gives you
the needed accuracy. You can also compare your budget to
other projects:
{registry_url}
            """
        )
        .finish()
    )


def make_column_config_block(
    name: str,
    statistic_name: StatisticName,
    lower_bound: float,
    upper_bound: float,
    bin_count: int,
):
    from dp_wizard.utils.code_generators.analyses import get_statistic_by_name

    return get_statistic_by_name(statistic_name).make_column_config_block(
        column_name=name,
        lower_bound=lower_bound,
        upper_bound=upper_bound,
        bin_count=bin_count,
    )
